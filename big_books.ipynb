{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "958aad8c-7f63-4f7b-96cc-b2b72eff48d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb7bd4ed-afe9-4b87-b492-fd8916fabc55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession, Row\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9445bb0-4cee-4b0d-8312-ccaa5f602416",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa7cc431-233e-4c91-ba58-5ff90ea7b2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_context_infos(context):\n",
    "    print(\"VERSION: \",context.version)\n",
    "    print(\"PYTHON_VERSION: \", context.pythonVer)\n",
    "    print(\"MASTER: \", context.master)\n",
    "    print(\"SPARK_HOME: \", str(context.sparkHome))\n",
    "    print(\"SPARK_USER: \", str(context.sparkUser()))\n",
    "    print(\"APP_NAME: \", context.appName)\n",
    "    print(\"APP_ID: \", context.applicationId)\n",
    "    print(\"DEFAULT_PARALLESLISM: \", context.defaultParallelism)\n",
    "    print(\"DEFAULT_PARTITION: \", context.defaultMinPartitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6627dbe-2d67-4016-ad22-649bb7a99329",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/manulabricole/Documents/CDN/BigData\n",
      "/Users/manulabricole/Documents/CDN/BigData/big_books/books1/epubtxt\n"
     ]
    }
   ],
   "source": [
    "cwd = os.getcwd()\n",
    "relative_books_path = \"big_books/books1/epubtxt\"\n",
    "books_path = os.path.join(cwd, relative_books_path)\n",
    "print(cwd)\n",
    "print(books_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a060395-ec78-4279-b9a3-6a4e60d8b31d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Web UI: http://localhost:4043\n",
      "NameNode Web UI: http://localhost:9870\n"
     ]
    }
   ],
   "source": [
    "spark_session = SparkSession.builder \\\n",
    "    .appName(\"hdfs_boks_session\") \\\n",
    "    .config(\"spark.driver.bindAddress\", \"127.0.0.1\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "print(\"Spark Web UI: http://localhost:4043\")\n",
    "print(\"NameNode Web UI: http://localhost:9870\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc3efe26-7329-431b-8041-26269db58b18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VERSION:  3.4.1\n",
      "PYTHON_VERSION:  3.11\n",
      "MASTER:  local[*]\n",
      "SPARK_HOME:  None\n",
      "SPARK_USER:  manulabricole\n",
      "APP_NAME:  PySparkShell\n",
      "APP_ID:  local-1690462251656\n",
      "DEFAULT_PARALLESLISM:  10\n",
      "DEFAULT_PARTITION:  2\n"
     ]
    }
   ],
   "source": [
    "spark_context = spark_session.sparkContext\n",
    "print_context_infos(spark_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b299fe-f745-4f63-b5a8-d732a9469d70",
   "metadata": {},
   "source": [
    "# Books Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5056331-f693-473d-9047-57835784a58f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of Books --> 17868\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for file in os.listdir(books_path):\n",
    "    if file.endswith(\".txt\"):\n",
    "        count += 1\n",
    "print(f\"Total Number of Books --> {count}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b45b3da-d435-46b8-9bce-759ce33ec982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of Paths --> 17868\n"
     ]
    }
   ],
   "source": [
    "text_list = os.listdir(books_path)\n",
    "text_list_path = []\n",
    "\n",
    "for text in text_list:\n",
    "    text_path = os.path.join(books_path, text)\n",
    "    text_list_path.append(text_path)\n",
    "\n",
    "print(f\"Total Number of Paths --> {len(text_list_path)}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7fe2945-63df-49de-a2dc-2dce951161db",
   "metadata": {},
   "source": [
    "### Count in 1 shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c3aaf9a-7b1b-4ce0-864b-f79c82be3d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_rdd = spark_session.sparkContext.wholeTextFiles(\"hdfs://localhost:9000/data_test/books/*\", minPartitions=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750c5742-ee7e-4fed-acf0-35ec8d9ca474",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:=>                                                  (647 + 10) / 17869]\r"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "words_rdd = text_rdd.flatMap(lambda line: line.split())\n",
    "num_words = words_rdd.count()\n",
    "\n",
    "end_words = time.time()\n",
    "numwords.take(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3ad273-e3ec-4e56-86c0-e30319309254",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "308373ee-f600-4daa-9aff-6451ed7f1d1c",
   "metadata": {},
   "source": [
    "### Break the process per books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f14dc2-288d-43f0-ba48-1ef0f9707d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_book(book_data):\n",
    "    file_path, file_content = book_data\n",
    "    # Your processing logic goes here, for example, replacing spaces with underscores\n",
    "    processed_content = file_content.replace(\" \", \"_\")\n",
    "    return (file_path, processed_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38ac44e-87ed-4e73-b005-066b83ad909d",
   "metadata": {},
   "outputs": [],
   "source": [
    "book_rdd = spark_session.sparkContext.wholeTextFiles(\"big_books/books1/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92637c94-e148-46b4-a822-fc0934265fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_rdd = book_rdd.map(process_book)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
