{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29511fb6-8298-4876-8d66-085922e1c149",
   "metadata": {},
   "source": [
    "# Import & Launch Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "933d36b3-ff00-4c15-a079-88445fb43f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import nltk\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "41157a86-0626-4cd1-b023-a05f6006b916",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession, Row\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558b8059-849e-4970-bd56-57f06201ed47",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dd524ce1-0332-4ac2-a11b-d24eea4dfe37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_context_infos(context):\n",
    "    print(\"VERSION: \",context.version)\n",
    "    print(\"PYTHON_VERSION: \", context.pythonVer)\n",
    "    print(\"MASTER: \", context.master)\n",
    "    print(\"SPARK_HOME: \", str(context.sparkHome))\n",
    "    print(\"SPARK_USER: \", str(context.sparkUser()))\n",
    "    print(\"APP_NAME: \", context.appName)\n",
    "    print(\"APP_ID: \", context.applicationId)\n",
    "    print(\"DEFAULT_PARALLESLISM: \", context.defaultParallelism)\n",
    "    print(\"DEFAULT_PARTITION: \", context.defaultMinPartitions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed16276-8f60-459d-96e0-a2e30c3e612c",
   "metadata": {},
   "source": [
    "## Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1cb17d38-9eba-4792-bd91-5e524fa7a249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/manulabricole/Documents/CDN/BigData\n"
     ]
    }
   ],
   "source": [
    "cwd = os.getcwd()\n",
    "print(cwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "412e4ff6-8253-4a37-8d69-1998a0600acf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Web UI: http://localhost:4040\n"
     ]
    }
   ],
   "source": [
    "spark_session = SparkSession.builder \\\n",
    "    .appName(\"book_session\") \\\n",
    "    .config(\"spark.driver.bindAddress\", \"127.0.0.1\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "print(\"Spark Web UI: http://localhost:4040\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "27975928-0d57-4d22-9a1a-eb5f689ac7f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VERSION:  3.4.1\n",
      "PYTHON_VERSION:  3.11\n",
      "MASTER:  local[*]\n",
      "SPARK_HOME:  None\n",
      "SPARK_USER:  manulabricole\n",
      "APP_NAME:  book_session\n",
      "APP_ID:  local-1690286868117\n",
      "DEFAULT_PARALLESLISM:  10\n",
      "DEFAULT_PARTITION:  2\n"
     ]
    }
   ],
   "source": [
    "spark_context = spark_session.sparkContext\n",
    "print_context_infos(spark_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec3b9e0-ce5d-4ab6-807f-6d6f524d62f1",
   "metadata": {},
   "source": [
    "## Import text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3afd264d-feaa-40aa-89e3-2f86cf92ee63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/manulabricole/Documents/CDN/BigData/beautifull_story.txt\n"
     ]
    }
   ],
   "source": [
    "filename = \"beautifull_story.txt\"\n",
    "text_path = os.path.join(cwd, filename)\n",
    "print(text_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e6c976ac-70bb-43ad-827e-6ea497bbc70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_rdd = spark_session.sparkContext.textFile(text_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2b545a7e-d900-47e6-9d01-c2e837378830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project Gutenberg's Beautiful Stories from Shakespeare, by E. Nesbit\n",
      "\n",
      "This eBook is for the use of anyone anywhere at no cost and with\n",
      "almost no restrictions whatsoever.  You may copy it, give it away or\n",
      "re-use it under the terms of the Project Gutenberg License included\n"
     ]
    }
   ],
   "source": [
    "for line in text_rdd.take(5):\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7141302-80b2-4993-bb24-ac446e3b8d4f",
   "metadata": {},
   "source": [
    "# Explore the book shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a7f3cc-273e-4551-a45e-74fff8cb0cd5",
   "metadata": {},
   "source": [
    "## Main infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7b79fdd1-46e6-4921-ad29-d99b71aea39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_lines = text_rdd.count()\n",
    "words_rdd = text_rdd.flatMap(lambda line: line.split())\n",
    "num_words = words_rdd.count()\n",
    "num_distinct_words = words_rdd.distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "59376135-2d46-4b37-929e-17fbb76fc218",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Lines           -->  7422\n",
      "Number of words           -->  52592\n",
      "Number of Different Words -->  10264\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of Lines           --> \", num_lines)\n",
    "print(\"Number of words           --> \", num_words)\n",
    "print(\"Number of Different Words --> \", num_distinct_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f761e2e-32d4-4df7-a34e-6a78ebfab60b",
   "metadata": {},
   "source": [
    "## Theme aborded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49569df6-e56e-4e6b-ad93-a05a3725f192",
   "metadata": {},
   "source": [
    "### Top 10 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "39eb4f13-dc99-4adc-8313-491bf978d96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the occurrences of each word\n",
    "word_counts_rdd = words_rdd.map(lambda word: (word, 1)).reduceByKey(lambda a, b: a + b)\n",
    "# Sort the words based on their count in descending order\n",
    "sorted_word_counts_rdd = word_counts_rdd.sortBy(lambda x: x[1], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "69270dc0-233c-483d-96d2-4f1381b1b60e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 2072), ('and', 1774), ('to', 1451), ('.', 1373), ('of', 1152), ('a', 937), ('he', 805), ('was', 762), ('his', 687), ('in', 638)]\n"
     ]
    }
   ],
   "source": [
    "# Take the first ten words from the sorted list\n",
    "top_10_words = sorted_word_counts_rdd.take(10)\n",
    "print(top_10_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f4bb33-0cb2-4cbc-9b86-e40ff3ab0bd4",
   "metadata": {},
   "source": [
    "### Top 10 words filtered"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a8269785-f6f9-45a4-9990-1a9e3511fcdd",
   "metadata": {},
   "source": [
    "We use the nltk library to prevent from catching useless words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6efc7d8c-8272-482b-90ae-2f3f5a4a941c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f475c75c-0a6b-4b69-8b95-ccdd8352a9d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/manulabricole/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/manulabricole/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/manulabricole/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "# Initialize NLTK and download the averaged_perceptron_tagger data package\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"averaged_perceptron_tagger\")\n",
    "nltk.download(\"punkt\")\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "fcf51e9d-f055-4f09-a0a3-bbc5737458f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_pattern(word):\n",
    "    # Define a regular expression pattern to remove unwanted characters\n",
    "    pattern = r'[^A-Za-z0-9]'\n",
    "    \n",
    "    # Use re.sub to remove unwanted characters from the word\n",
    "    cleaned_word = re.sub(pattern, '', word)\n",
    "    \n",
    "    return cleaned_word\n",
    "\n",
    "# Function to check if a word is a verb using NLTK's part-of-speech tagging\n",
    "def is_verb(word):\n",
    "    tagged_word = pos_tag([word])\n",
    "    return tagged_word[0][1].startswith(\"VB\")\n",
    "\n",
    "# Function to filter out unwanted characters and check if a word is a verb\n",
    "def is_valid_word(word):\n",
    "    word = clean_pattern(word)\n",
    "    word = word.lower()\n",
    "    return word.isalpha() and word not in stop_words and not is_verb(word)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f79f8114-5e5d-48cb-9d3b-ce6b17ed931e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 69:>                                                         (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We pass from 52592 to 20782 words ! \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Tokenize each line by splitting it into words, remove stop words and unwanted characters\n",
    "words_rdd = text_rdd.flatMap(lambda line: word_tokenize(line)).filter(is_valid_word)\n",
    "new_number = words_rdd.count()\n",
    "print(f\"We pass from {num_words} to {new_number} words ! \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c836b268-8b91-4d1b-a165-346ace52aeac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Count the occurrences of each word. reduceByKLey receive a function. \n",
    "# For the same key, a and b are the values coming from two key value pair. We keep the common key but make the sum of the values\n",
    "word_counts_rdd = words_rdd.map(lambda word: (word, 1)).reduceByKey(lambda a, b: a + b)\n",
    "# Sort the words based on their count in descending order\n",
    "sorted_word_counts_rdd = word_counts_rdd.sortBy(lambda x: x[1], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f06d1108-a150-4969-803f-355d6194ebc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "King: 183\n",
      "would: 174\n",
      "love: 146\n",
      "Duke: 124\n",
      "one: 111\n",
      "man: 110\n",
      "could: 106\n",
      "told: 103\n",
      "father: 88\n",
      "like: 85\n",
      "Project: 83\n",
      "wife: 82\n",
      "Claudio: 73\n",
      "thought: 71\n",
      "Timon: 70\n",
      "two: 68\n",
      "Othello: 68\n",
      "Macbeth: 66\n",
      "daughter: 65\n",
      "day: 64\n"
     ]
    }
   ],
   "source": [
    "for word, count in sorted_word_counts_rdd.take(20):\n",
    "    print(f\"{word}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26190f90-cb97-4413-a22c-dfd86be2b2f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
